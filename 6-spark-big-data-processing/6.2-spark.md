# 6.2 Spark

### Revisão do conteúdo

Na segunda aula de Spark teve a apresentação do Jupyter Notebook e alguns atalhos úteis; criação de sessão no Spark; API Catalog - Diferenças entre Scala e Python e Preparação do ambiente (download dos arquivos que serão utilizados nesse módulo).

{% hint style="info" %}
&#x20;Mais informações estão disponibilizadas na [documentação oficial](https://spark.apache.org/docs/latest/).
{% endhint %}

### Exercícios

Antes de iniciar os exercícios, é necessário ativar o cluster

`cd treinamentos`\
`cd spark`\
`docker-compose -f docker-compose-parcial.yml start`

**1. Configurar o jar do spark para aceitar o formato parquet em tabelas Hive**

Para realizar a configuração, realizar download do arquivo disponibilizado no primeiro link e em seguida copiar o arquivo para a pasta de configuração.\
Utilizar os comandos abaixo:

`curl -O https://repo1.maven.org/maven2/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar`

`docker cp parquet-hadoop-bundle-1.6.0.jar jupyter-spark:/opt/spark/jars`

**2. Baixar os dados dos exercícios do treinamento no diretório spark/input (volume no Namenode)**

Os exercícios serão copiados do repositório github do professor para a pasta input, contida no diretório spark.

* Nota: É necessário utilizar o comando sudo pois o _cluster_ está em execução. A senha (_password_) foi definida na etapa de instalação do Ubuntu.
* Nota: O diretório _input_ é mapeado como volume no _namenode_

`cd input`\
`sudo git clone https://github.com/rodrigo-reboucas/exercises-data.git`

![](../.gitbook/assets/m6\_aula2\_00.png)

**3. Verificar o envio dos dados para o namenode**

Para acessar o namenode, utilizar o comando `docker exec -it namenode bash`Com os comando `ls` listar o conteúdo dos diretórios.

![](../.gitbook/assets/m6\_aula2\_01.png)

**4. Criar no hdfs a seguinte estrutura: /user/danielle/data**

Para realizar essa etapa, vamos incialmente listar todas as pastas que estão na pasta _user_ utilizando o seguinte comando: `hdfs dfs -ls -R /user`           &#x20;

Observa-se que não existe a pasta danielle, então deverá ser criada utilizando os seguintes comandos:&#x20;

`hdfs dfs -mkdir -p /user/danielle/data`                                                     &#x20;

Para verificar se os subdiretórios forma criados adequadamente, utilizar novamente o comando `hdfs dfs -ls -R /user`&#x20;

![](../.gitbook/assets/m6\_aula2\_02.png)

**5. Enviar todos os dados do diretório input para o hdfs em /user/danielle/data**

Para enviar os arquivos, utiliza-se o comando -put. Notar que os arquivos estão dentro da pasta _exercises-input_. Dado os caminhos descritos acima, os comandos a serem utilizado são:&#x20;

`hdfs dfs -put /input/exercises-data/* /user/danielle/data`\
\
Para visualizar a lista dos documentos movidos da pasta escola, utilizar o comando`hdfs dfs -ls /user/danielle/data`

![](../.gitbook/assets/m6\_aula2\_03.png)

### Exercícios – Testar o Jupyter Notebook

Acessar [http://localhost:8889](http://localhost:8889)

**1. Criar o arquivo do notebook com o nome teste\_spark.ipynb**

Em new selecionar a opção PySpark. Na nova janela, renomear o arquivo - clicando  no título.

**2. Obter as informações da sessão de spark (spark)**

![](../.gitbook/assets/m6\_aula2\_04.png)

**3. Obter as informações do contexto do spark (sc)**

![](../.gitbook/assets/m6\_aula2\_05.png)

**4. Setar o log como INFO.**

Para realizar essa tarefa, utilizar o comando `spark.sparkContext.setLogLevel("INFO")`

**5. Visualizar todos os banco de dados com o catalog**

Para realizar essa tarefa, utilizar o comando: `spark.catalog.listDatabases()`

`Output (saída):`&#x20;

```
[Database(name='default', description='Default Hive database', locationUri='hdfs://namenode:8020/user/hive/warehouse')]
```

**6. Ler os dados "hdfs://namenode:8020/user/rodrigo/data/juros\_selic/juros\_selic.json“ com uso de Dataframe**

Executar a célula com os seguintes comandos:

`tab_juros = spark.read.json("caminho do arquivo")` \
****`tab_juros.show(10)`

![](../.gitbook/assets/m6\_aula2\_06.png)

**7. Salvar o Dataframe como juros no formato de tabela Hive**

Para salvar no Hive, usar os seguintes comandos:

`tab_juros.write.saveAsTable("juros")`

**8. Visualizar todas as tabelas com o catalog**

Executar a célula com os seguintes comandos:\
****`spark.catalog.listTables()`

Output (saída):&#x20;

```
Table(name='juros', database='default', description=None, tableType='MANAGED', isTemporary=False),
```

**9. Visualizar no hdfs o formato e compressão que está a tabela juros do Hive**

Executar a célula com os seguintes comandos:`!hdfs dfs -ls /user/hive/warehouse/juros`

Output (saída):

```
Found 1 items
drwxr-xr-x   - root supergroup          0 2021-06-24 23:39 /user/hive/warehouse/juros
```

**10. Ler e visualizar os dados da tabela juros, com uso de Dataframe no formato de Tabela Hive**

Executar a célula com os seguintes comandos:`spark.read.table("juros").show(10)`

![](../.gitbook/assets/m6\_aula2\_07.png)

**11. Ler e visualizar os dados da tabela juros , com uso de Dataframe no formato Parquet**

Executar a célula com os seguintes comandos:`spark.read.parquet("/user/hive/warehouse/juros/").show(10)`

![](../.gitbook/assets/m6\_aula2\_08.png)

O _notebook_ dessa aula está disponibilizado abaixo:

