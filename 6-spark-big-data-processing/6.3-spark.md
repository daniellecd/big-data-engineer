# 6.3 Spark

### Revisão do conteúdo

Na terceira aula de Spark foram presentados os conceitos de RDD (Resilient Distributed Datasets); leitura e manipulação de dados; partições do RDD.

{% hint style="info" %}
&#x20;Mais informações estão disponibilizadas na [documentação oficial](https://spark.apache.org/docs/latest/). Conteúdo específico sobre RDD está disponibilizado [aqui](https://spark.apache.org/docs/latest/rdd-programming-guide.html).
{% endhint %}

As operações em RDD são divididas em dois tipos, de transformação e de ação.&#x20;

* Transformação: retornam um novo RDD
* Ação: retornam um resultado para o driver ou escreve na camada de armazenamento.

Alguns exemplos de operações de transformação:&#x20;

* **map (func)** - retorna um novo RDD aplicando a função func em cada elemento.&#x20;
* **filter (func)** - retorna um novo RDD aplicando o filtro func.&#x20;
* **flatMap (func)** - similar ao map, mas retornando mais itens ao invés de apenas um.&#x20;
* **sample(withReplacement, fração, semente)** - amostra aleatoriamente uma fração dos dados com ou sem reposição usando a semente para gerar os números aleatórios.&#x20;
* **union(rdd)** - retorna um novo RDD que contém a união dos elementos do RDD original e do RDD passado como argumento.&#x20;
* **distinct()** - retorna um novo dataset contendo os valores distintos do RDD original.&#x20;
* **groupByKey()** - aplicado em pair RDD’s da forma (K, V), retornando um novo pair RDD da forma (K, iterable).&#x20;
* **reduceByKey(func)** - aplicado também em um pair RDD (K, V), agregando os valores de V pela função func para cada chave K.&#x20;
* **pipe(command)** - aplica para cada partição do RDD um comando shell. Qualquer linguagem com stdin, stdout pode ser utilizada.&#x20;

Algumas operações de ação:&#x20;

* **collect( )** - retorna todos os elementos do RDD como um array para o driver. Usado principalmente após uma operação de filtro para retornar poucos dados.&#x20;
* **count( )** - retorno o número de elementos no RDD.&#x20;
* **first( )** - retorna o primeiro elemento do dataset.&#x20;
* **take(n)** - retorna os primeiros n elementos do dataset.&#x20;
* **saveAsTextFile(file)** - salva o RDD em um arquivo de texto.

****[**Referência**](https://www.ime.usp.br/\~amaris/mac-5742/reports/ApacheSpark.pdf)****

### **Exercícios - RDD**

Antes de iniciar os exercícios, é necessário ativar o cluster e acessar o localhost do Jupyter

`cd treinamentos`\
`cd spark`\
`docker-compose -f docker-compose-parcial.yml start`\
****`http://localhost:8889/`

* **Nota:** Para os exercícios realizados no jupyter notebook, abaixo de cada questão será incluído apenas o comando necessário. As saídas (outputs) estarão disponíveis no arquivo .ipynb no fim da página.

**1.Ler com RDD os arquivos localmente do diretório “/opt/spark/logs/” ("file:///opt/spark/logs/")**

`rdd = sc.textFile("file:///opt/spark/logs/")`

**2. Com uso de RDD faça as seguintes operações**

a) Contar a quantidade de linhas

`rdd.count()`

b) Visualizar a primeira linha

`rdd.first()`

c) Visualizar todas as linhas

`rdd.collect()`

d) Contar a quantidade de palavras

`palavras = rdd.flatMap(lambda linha: linha.split(" "))` \
`palavras.count()`

e) Converter todas as palavras em minúsculas

`minuscula = palavras.map(lambda palavra : palavra.lower())`

f) Remover as palavras de tamanho menor que 2

`tamanho = minuscula.filter(lambda palavra: len(palavra)>2)`

g) Atribuir o valor de 1 para cada palavra

`valor = tamanho.map (lambda palavra: (palavra,1))`

h) Contar as palavras com o mesmo nome

`mesmo_nome = valor.reduceByKey(lambda chave1, chave2: chave1 + chave2)`

i) Visualizar em ordem alfabética

`alfabetica = mesmo_nome.sortBy(lambda palavra: palavra[0])`

j) Visualizar em ordem decrescente a quantidade de palavras

`quantidade = mesmo_nome.sortBy(lambda palavra: palavra[1], False)`

k) Remover as palavras, com a quantidade de palavras > 1

`mantidas = quantidade.filter(lambda palavra: palavra[1]>1)`

l) Salvar o RDD no diretorio do HDFS /user/\<seu-nome>/logs\_count\_word

`mantidas.saveAsTextFile("/user/danielle/logs_count_word")`

{% file src="../.gitbook/assets/Aula 03 - Exercício.html" %}
Arquivo HTML - Aula 03
{% endfile %}

{% file src="../.gitbook/assets/Aula 03 - Exercício.zip" %}
Arquivo .ipynb - Aula 03
{% endfile %}

### Exercícios - RDD com Partições

1\. Ler com RDD os arquivos localmente do diretório “/opt/spark/logs/” ("file:///opt/spark/logs/") com 10 partições

`rdd = sc.textFile("file:///opt/spark/logs/", 10)`

2\. Contar a quantidade de cada palavras em ordem decrescente do RDD em 5 partições



3\. Salvar o RDD no diretório do HDFS /user/\<seu-nome>/logs\_count\_word\_5



4\. Refazer a questão 2, com todas as funções na mesma linha de um RDD
