# Semantix

[Gitbook desse projeto](https://daniellecd.gitbook.io/big-data-engineer/)

## 1. Conteúdo programático

O Treinamento tem duração de 11 semanas \(19/04/2021 à 02/07/2021\) e formado por 6 módulos, com a seguinte divisão:

📚 **Big Data Foundations \(Semana 1, 2 e 3\):**

* Conhecimento de ferramentas atuais no mercado de Big Data
* Criação e funcionamento de um cluster Hadoop para Big Data em Docker
* Manipulação de dados com HDFS
* Manipulação de dados com uso do Hive
* Otimização de consultas em grandes volumes de dados estruturados e semiestruturados com uso de Hive
* Ingestão de dados relacionais para o HDFS/Hive, com uso do Sqoop
* Otimização de importação no Sqoop
* Exportação de dados do HDFS para o SGBD, com uso do Sqoop
* Manipulação de dados com HBase
* Operações com Dataframe em Spark para processamento de dados em batch
* Uso do Spark SQL Queries para consultas de dados estruturados e semiestruturados

📚 **MongoDB - Básico \(Semana 4\):**

* Entendimento de conceitos e arquitetura NoSQL e MongoDB
* Instalação de cluster MongoDB através de container e Cloud
* Manipular coleções, documentos e índices
* Realizar diversas pesquisas no MongoDB com diferentes operadores
* Fazer uso das interfaces gráficas MongoExpress e MongoCompass
* Trabalhar com pipeline de agregações
* Entendimento de Replicação e shards

📚 **Redis – Básico \(Semana 5\):**

* Entendimento de conceitos e arquitetura NoSQL e Redis
* Instalação de cluster Redis através de container
* Manipulação de diversos tipos de estrutura de dados com Redis-CLI
* Implementar paradigma de mensagens Pub/Sub
* Configurações básicas de persistência de dados

📚 **Kafka – Básico \(Semana 6\):**

* Entendimento de conceitos e arquitetura do Kafka e da Confluent
* Instalação de cluster Kafka através de container
* Gerenciamento de tópicos
* Produção e consumo de dados através do console
* Entendimento das guias do Control Center
* Desenvolvimento de stream com uso do KSQL
* Aplicação de KSQL Datagen
* Produção e consumo de dados com uso do Schema Registry
* Trabalhando com Kafka Connect
* Custos com Confluent Cloud
* Otimização de parâmetros
* Melhores práticas em um cluster Kafka

📚 **Elastic Essential I \(Semana 7 e 8\):**

* Entendimento de conceitos e arquitetura da Elastic
* Instalação de cluster Elastic através de container
* Realizar operações de CRUD em índices
* Gerenciamento de índices
* Alteração de mapeamento e reindex
* Desenvolvimento de consultas do tipo term, terms, range, match e multi\_match, com uso de bool query
* Aplicação de analyzers em atributos
* Desenvolvimento de agregações básicas
* Ingestão de dados através de beats e logstash
* Entendimento das guias do Kibana

📚 **Spark - Big Data Processing \(Semana 9, 10 e 11**\)

* Uso do Jupyter Notebooks para a criação de projetos em Spark com Python
* Spark batch intermediario
* Operações com RDD em Spark para processamento de dados em batch
* Uso de Partições com RDD
* Operações com Dataset em Spark para processamento de dados em batch
* Uso de Dataset em Dataframe e RDD
* Comandos avançados com Dataset
* Uso do IntelliJ IDEA para a criação de projetos em Spark com Scala
* Struct Streaming para leitura de dados do Kafka
* Spark Streaming para leitura de dados do Kafka
* Otimizações com uso de Variáveis Compartilhadas
* Criações de User defined Function
* Configurações de Tunning para o Spark Application.

## 2. Requisitos mínimos para o treinamento:

Conhecimento intermediário de pelo menos uma destas linguagens:

* Python \(Preferível\)
* Scala
* Java
* Conhecimento intermediário em SQL
* Conhecimento básico em Git

## 3. Computador necessário para o treinamento:

* Sistema Operacional de 64 bits
* Memória RAM de 8 GB
* Acesso a internet
* HD com no mínimo 30 GB de espaço livre

## 4. Avaliação:

Após conclusão de cada módulo do treinamento, o aluno irá obter uma insígnia online, caso conclua 100% do módulo e tire uma nota superior a 7 na avaliação.

#### 

#### Feito com ♥ by [daniellecd](https://github.com/daniellecd)

