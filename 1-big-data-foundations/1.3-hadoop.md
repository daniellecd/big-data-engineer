# 1.3 Hadoop

### Revisão do conteúdo

Nessa aula foram apresentadas as características principais _Hadoop Distributed File System_ (HDFS - Sistema de Arquivos Distribuídos do Hadoop) e algumas operações básicas de manipulação de dados, como criar diretórios, mover, remover, copiar e listar diretórios.

{% hint style="info" %}
Os comandos do HDFS estão disponibilizados na [documentação oficial](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html) e, de forma resumida, nesse arquivo.
{% endhint %}

### Exercício - Comandos HDFS

**1. Iniciar o cluster de Big Data**

Primeiramente deve-se acessar o diretório criado anteriormente (_treinamentos_) e iniciar o cluster com os comandos apresentados abaixo:

`cd treinamentos` \
`cd docker-bigdata`\
`docker-compose up -d`

![](../.gitbook/assets/m1\_aula3\_00.png)

**2. Baixar os dados dos exercícios do treinamento**

Os exercícios serão copiados do [repositório github](https://github.com/rodrigo-reboucas/exercises-data.git) do professor para a pasta input, contida no diretório docker-bigdata.

Nota: É necessário utilizar o comando sudo pois o _cluster_ está em execução. A senha (_password_) foi definida na etapa de instalação do Ubuntu.

`cd input`\
`sudo git clone https://github.com/rodrigo-reboucas/exercises-data.git`

![](../.gitbook/assets/m1\_aula3\_01.png)

**3. Acessar o container do namenode**

Para acessar o namenode, utilizar o comando`docker exec -it namenode bash`

![](<../.gitbook/assets/m1\_aula3\_02 (1).png>)

**4. Criar a estrutura de pastas apresentada abaixo pelo comando:**&#x20;

`hdfs dfs -ls -R /user/aluno/<nome>/data`\
`hdfs dfs -ls -R /user/aluno/<nome>/recover`\
`hdfs dfs -ls -R /user/aluno/<nome>/delete`

Para realizar essa etapa, vamos incialmente listar todas as pastas que estão na pasta _user_ utilizando o seguinte comando: `hdfs dfs -ls -R /user`              &#x20;

![](../.gitbook/assets/m1\_aula3\_03.png)

Observa-se que não existe a pasta aluno, então deverá ser criada utilizando os seguintes comandos:&#x20;

`hdfs dfs -mkdir -p /user/aluno/<nome>/data`\
`hdfs dfs -mkdir  /user/aluno/<nome>/recover`\
`hdfs dfs -mkdir /user/aluno/<nome>/delete`                                                        &#x20;

Nota: Para a criação das pastas _recover_ e _delete_ não é necessário acrescentar o `-p` pois a estrutura do diretório já havia sido definida anteriormente.

Para verificar se os subdiretórios forma criados adequadamente, utilizar o comando `hdfs dfs -ls -R /user/aluno`&#x20;

![](../.gitbook/assets/m1\_aula3\_04.png)

Dica: É possível utilizar um único comando para criar as três pastas (subdiretórios) `hdfs dfs -mkdir -p /user/aluno//{data,recover,delete}`

**5. Enviar a pasta “/input/exercises-data/escola” e o arquivo “/input/exercises-data/entrada1.txt” para data.**

Para enviar os arquivos, utiliza-se o comando -put. Dado os caminhos descritos acima, os comandos a serem utilizado são:&#x20;

`hdfs dfs -put /input/exercises-data/escola/ /user/aluno/danielle/data`\
``\
`hdfs dfs -put /input/exercises-data/entrada1.txt/ /user/aluno/danielle/data`

Para visualizar a lista dos documentos movidos da pasta escola, utilizar o comando`hdfs dfs -ls -R /user/aluno/danielle/data` e para verificar se a pasta escola e entrada1.txt estão dentro de data - sem detalhar cada ,  utilizar comando semelhante ao anterior, mas sem o `-R` (recursivo) .

![](../.gitbook/assets/m1\_aula3\_05.png)

![](../.gitbook/assets/m1\_aula3\_06.png)

**6. Mover o arquivo “entrada1.txt” para recover**

Para enviar os arquivos, utiliza-se o comando -mv. Dado os caminhos descritos no enunciado, os comandos a serem utilizado são:&#x20;

`hdfs dfs -mv /user/aluno/danielle/data/entrada1.txt /user/aluno/danielle/recover`

Para visualizar se o documentos foi movidos da pasta recover, utilizar o comando`hdfs dfs -ls -R /user/aluno/danielle/recover`

![](../.gitbook/assets/m1\_aula3\_07.png)

**7. Baixar o arquivo do hdfs “escola/alunos.json” para o sistema local /**

Para mover o arquivo do HDFS para ambiente local, é necessário utilizar o seguinte comando:

`hdfs dfs -get /user/aluno/danielle/data/escola/alunos.json /home/danielle`

**8. Deletar a pasta recover**

O comando para deletar é o `-rm` e o `-R` é necessário pois se trata de de um diretório. Para deletar a pasta _recover_, utiliza-se o seguinte código:

`hdfs dfs -rm -R /user/aluno/danielle/recover`

![](<../.gitbook/assets/m1\_aula3\_08 (1).png>)

**9. Deletar permanentemente o delete**

O comando para deletar permanentemente, isto é, sem ir para lixeira é o `-skipTrash,` adicional aos comandos mostrados na questão anterior. Para deletar a pasta delete, utiliza-se o seguinte código:

`hdfs dfs -rm -skipTrash -R /user/aluno/danielle/delete`

![](<../.gitbook/assets/m1\_aula3\_09 (1).png>)

**10. Procurar o arquivo “alunos.csv” dentro do /user**

A busca de arquivos é realizada com o comando `-find`. Para localizar o arquivo, utiliza-se o seguinte código:

`hdfs dfs -find /user -name alunos.csv`

![](../.gitbook/assets/m1\_aula3\_10.png)

**11. Mostrar o último 1KB do arquivo “alunos.csv”**

`hdfs dfs -tail /user/aluno/danielle/data/escola/alunos.csv`

![](../.gitbook/assets/m1\_aula3\_11.png)

**12. Mostrar as 2 primeiras linhas do arquivo “alunos.csv”**

`hdfs dfs -cat /user/aluno/danielle/data/escola/alunos.csv | head -n 2`

![](../.gitbook/assets/m1\_aula3\_12.png)

**13. Verificação de soma das informações do arquivo “alunos.csv”**

`hdfs dfs -checksum /user/aluno/danielle/data/escola/alunos.csv`

![](../.gitbook/assets/m1\_aula3\_13.png)

**14. Criar um arquivo em branco com o nome de “test” no data**

`hdfs dfs -touchz /user/aluno/danielle/data/test`

`hdfs dfs -ls /user/aluno/danielle/data/`

![](../.gitbook/assets/m1\_aula3\_14.png)

**15. Alterar o fator de replicação do arquivo “test” para 2**

`hdfs dfs -setrep 2 /user/aluno/danielle/data/test`

`hdfs dfs -ls /user/aluno/danielle/data/`

![](<../.gitbook/assets/m1\_aula3\_15 (1).png>)

**16. Ver as informações do arquivo “alunos.csv”**

`hdfs dfs -stat %r /user/aluno/danielle/data/escola/alunos.csv`\
`hdfs dfs -stat %o /user/aluno/danielle/data/escola/alunos.csv`\
`hdfs dfs -stat %u /user/aluno/danielle/data/escola/alunos.csv`\
`hdfs dfs -stat %n /user/aluno/danielle/data/escola/alunos.csv`

![](../.gitbook/assets/m1\_aula3\_16.png)

**17. Exibir o espaço livre do data e o uso do disco**

`hdfs dfs -df /user/aluno/danielle/data`\
`hdfs dfs -df -h /user/aluno/danielle/data`\
`hdfs dfs -du /user/aluno/danielle/data`\
`hdfs dfs -du -h /user/aluno/danielle/data`\
`hdfs dfs -du -h /`

![](../.gitbook/assets/m1\_aula3\_17.png)
